{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"vinai/phobert-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing BertModel: ['roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.embeddings.position_ids', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'lm_head.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "e:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2357: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   218,    52,   222,    25,   212,  2919,   222,   385,  8249,\n",
      "             2],\n",
      "        [    0,   218,     8,   277,     2,     1,     1,     1,     1,     1,\n",
      "             1],\n",
      "        [    0,   716,   306, 17591, 18657,  8210,  1517,  1157,  1476,     2,\n",
      "             1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}\n",
      "torch.Size([3, 11])\n",
      "output odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "config = BertConfig.from_pretrained(\"vinai/phobert-base-v2\", output_hidden_states=True)\n",
    "model = BertModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "inputs = tokenizer([\"Tôi đang học ở trường đại học tự nhiên\", \"Tôi là ai\"], return_tensors=\"pt\", padding=True, max_length=max)\n",
    "inputs2 = tokenizer.batch_encode_plus([\"Tôi đang học ở trường đại học tự nhiên\", \"Tôi là ai\", \"Hai tay bưng dĩa í a bánh bò\"], return_tensors=\"pt\", padding=True, max_length=512, truncation=True, add_special_tokens=True)\n",
    "print(inputs2)\n",
    "print(inputs2['input_ids'].shape)\n",
    "outputs = model(**inputs2)\n",
    "print(\"output\", outputs.keys())\n",
    "print(len(outputs))  # 3\n",
    "\n",
    "hidden_states = outputs['last_hidden_state']\n",
    "print(len(hidden_states))  # 13\n",
    "\n",
    "embedding_output = hidden_states[0]\n",
    "attention_hidden_states = hidden_states[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import torch\n",
    "filetrain='train_pharagraph_full.jl'\n",
    "filetarget='target_strings_full.jl'\n",
    "train_pharagraph = joblib.load(filetrain)\n",
    "target_strings = joblib.load(filetarget)\n",
    "\n",
    "targets = []\n",
    "for ele in target_strings:\n",
    "    ele = re.sub(r'[\\W_]', ' ', ele)\n",
    "    ele = re.sub(r'[^\\w\\s]', '', ele)\n",
    "    ele = re.sub(r'\\t\\n', '', ele)\n",
    "    targets.append(ele)\n",
    "\n",
    "training_input = []\n",
    "for ele in train_pharagraph:\n",
    "    ele = re.sub(r'[\\W_]', ' ', ele)\n",
    "    ele = re.sub(r'[^\\w\\s]', '', ele)\n",
    "    ele = re.sub(r'\\t\\n', '', ele)\n",
    "    training_input.append(ele)\n",
    "\n",
    "document = pd.Series(training_input)\n",
    "summary = pd.Series(targets)\n",
    "\n",
    "doc_list = []\n",
    "sum_list = []\n",
    "for index, doc in enumerate(document):\n",
    "    if len(doc) < 2000:\n",
    "        doc_list.append(doc)\n",
    "        sum_list.append(summary[index])\n",
    "\n",
    "document = pd.Series(doc_list)\n",
    "summary = pd.Series(sum_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2357: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "targets = tokenizer(sum_list[:100],  return_tensors=\"pt\", padding=True,max_length=200)\n",
    "inputs = tokenizer(doc_list[:100],  return_tensors=\"pt\", padding=True,max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, inputs,targets):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inp_input_ids = self.inputs['input_ids'][index]\n",
    "        inp_token_type_ids = self.inputs['token_type_ids'][index]\n",
    "        inp_attention_mask = self.inputs['attention_mask'][index]\n",
    "        tar_input_ids = self.targets['input_ids'][index]\n",
    "        tar_token_type_ids = self.targets['token_type_ids'][index]\n",
    "        tar_attention_mask = self.targets['attention_mask'][index]\n",
    "\n",
    "        return {\n",
    "            'inp_input_ids' : inp_input_ids,\n",
    "            'inp_token_type_ids' : inp_token_type_ids,\n",
    "            'inp_attention_mask' : inp_attention_mask,\n",
    "            'tar_input_ids' : tar_input_ids,\n",
    "            'tar_token_type_ids' : tar_token_type_ids,\n",
    "            'tar_attention_mask' : tar_attention_mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(inputs, targets)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 16894,  3139,   963,  2271,    48,    99,  5330, 11603,     6,\n",
      "           48,   358,   113,    80,     7,  1393,  2075,    24,   273,   123,\n",
      "          188,  2006, 46779,     2])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(tar_input_ids)\n\u001b[0;32m     27\u001b[0m inp_input_ids,inp_token_type_ids,inp_attention_mask, tar_input_ids \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39minp_input_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), row[\u001b[39m'\u001b[39m\u001b[39minp_token_type_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), row[\u001b[39m'\u001b[39m\u001b[39minp_attention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device), row[\u001b[39m'\u001b[39m\u001b[39mtar_input_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 28\u001b[0m train_step( inp_input_ids\u001b[39m.\u001b[39;49mto(device), inp_token_type_ids\u001b[39m.\u001b[39;49mto(device), inp_attention_mask\u001b[39m.\u001b[39;49mto(device), tar_input_ids\u001b[39m.\u001b[39;49mto(device) , transformer, optimizer, scheduler)\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\main.py:105\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(inp_input_ids, inp_token_type_ids, inp_attention_mask, tar_input_ids, transformer, optimizer, scheduler)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(inp_input_ids, inp_token_type_ids, inp_attention_mask, tar_input_ids, transformer, optimizer, scheduler):\n\u001b[0;32m    104\u001b[0m     tar_real \u001b[39m=\u001b[39m tar_input_ids\n\u001b[1;32m--> 105\u001b[0m     enc_padding_mask, combined_mask, dec_padding_mask \u001b[39m=\u001b[39m create_masks(inp_input_ids, tar_input_ids)\n\u001b[0;32m    106\u001b[0m     predictions, enc_output, att_weights \u001b[39m=\u001b[39m transformer(\n\u001b[0;32m    107\u001b[0m         inp_input_ids, inp_token_type_ids, inp_attention_mask, tar_input_ids, \n\u001b[0;32m    108\u001b[0m         enc_padding_mask, \n\u001b[0;32m    109\u001b[0m         combined_mask, \n\u001b[0;32m    110\u001b[0m         dec_padding_mask,\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    112\u001b[0m     loss \u001b[39m=\u001b[39m loss_function(tar_real, predictions)\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\helper.py:116\u001b[0m, in \u001b[0;36mcreate_masks\u001b[1;34m(inp, tar)\u001b[0m\n\u001b[0;32m    114\u001b[0m dec_padding_mask \u001b[39m=\u001b[39m create_padding_mask(inp)\n\u001b[0;32m    115\u001b[0m look_ahead_mask \u001b[39m=\u001b[39m create_look_ahead_mask(tar\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[1;32m--> 116\u001b[0m look_ahead_mask \u001b[39m=\u001b[39m look_ahead_mask\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m    117\u001b[0m dec_target_padding_mask \u001b[39m=\u001b[39m create_padding_mask(tar)\n\u001b[0;32m    119\u001b[0m combined_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(dec_target_padding_mask, look_ahead_mask)\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m\"\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "from transform import *\n",
    "word2Topic = joblib.load('word2Topic_v2.jl')\n",
    "list_topic_count = joblib.load('list_topic_count.jl')\n",
    "\n",
    "encoder_vocab_size = tokenizer.vocab_size\n",
    "decoder_vocab_size = tokenizer.vocab_size\n",
    "transformer = TransformerModel(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    encoder_vocab_size, \n",
    "    decoder_vocab_size, \n",
    "    pe_input=encoder_vocab_size, \n",
    "    pe_target=decoder_vocab_size,\n",
    "    word2Topic=word2Topic,\n",
    "    list_topic_count=list_topic_count,\n",
    "    bert=model\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "learning_rate = CustomSchedule(optimizer, d_model)\n",
    "logging.info(f\"learning rate - {learning_rate}\")\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: epoch/10)\n",
    "for batch, row in enumerate(dataloader):\n",
    "            print(tar_input_ids)\n",
    "            inp_input_ids,inp_token_type_ids,inp_attention_mask, tar_input_ids = row['inp_input_ids'].to(device), row['inp_token_type_ids'].to(device), row['inp_attention_mask'].to(device), row['tar_input_ids'].to(device)\n",
    "            train_step( inp_input_ids.to(device), inp_token_type_ids.to(device), inp_attention_mask.to(device), tar_input_ids.to(device) , transformer, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from config import *\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, inputs,targets):\n",
    "        super().__init__()\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inp_input_ids = self.inputs['input_ids'][index]\n",
    "        inp_token_type_ids = self.inputs['token_type_ids'][index]\n",
    "        inp_attention_mask = self.inputs['attention_mask'][index]\n",
    "        tar_input_ids = self.targets['input_ids'][index]\n",
    "        tar_token_type_ids = self.targets['token_type_ids'][index]\n",
    "        tar_attention_mask = self.targets['attention_mask'][index]\n",
    "\n",
    "        return {\n",
    "            'inp_input_ids' : inp_input_ids,\n",
    "            'inp_token_type_ids' : inp_token_type_ids,\n",
    "            'inp_attention_mask' : inp_attention_mask,\n",
    "            'tar_input_ids' : tar_input_ids,\n",
    "            'tar_token_type_ids' : tar_token_type_ids,\n",
    "            'tar_attention_mask' : tar_attention_mask,\n",
    "        }\n",
    "\n",
    "# dataset = MyDataset(inputs,targets)\n",
    "# dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from helper import *\n",
    "from config import *\n",
    "\n",
    "dataset = MyDataset(inputs,targets)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inp_input_ids', 'inp_token_type_ids', 'inp_attention_mask', 'tar_input_ids', 'tar_token_type_ids', 'tar_attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "for batch, inp in enumerate(dataset):\n",
    "    print(inp.keys())\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss = []\n",
    "def train_step(inp, tar, transformer, optimizer, scheduler):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "    predictions, enc_output, att_weights = transformer(\n",
    "        inp, tar_inp, \n",
    "        True, \n",
    "        enc_padding_mask, \n",
    "        combined_mask, \n",
    "        dec_padding_mask,\n",
    "    )\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at vinai/phobert-base-v2 were not used when initializing BertModel: ['roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'lm_head.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['embeddings.token_type_embeddings.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'pooler.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m      8\u001b[0m decoder_vocab_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mget_vocab()\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m      9\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(\n\u001b[0;32m     10\u001b[0m     num_layers, \n\u001b[0;32m     11\u001b[0m     d_model, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     bert\u001b[39m=\u001b[39mmodel\n\u001b[0;32m     17\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mtransformer_epoch_15.pt\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     19\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[39m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[39mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmmap can only be used with files saved with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1440\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.load.metadata\u001b[39m\u001b[39m\"\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mserialization_id\u001b[39m\u001b[39m\"\u001b[39m: zip_file\u001b[39m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1408\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1410\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1377\u001b[0m         storage\u001b[39m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1379\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1382\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1383\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1384\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1386\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1387\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    390\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 391\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    392\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    265\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    267\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    268\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32me:\\Code Summary\\summarize-torch\\.venv\\lib\\site-packages\\torch\\serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    247\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    251\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    252\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    253\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    254\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    255\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "from main import *\n",
    "from transform import *\n",
    "from config import *\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "model = BertModel.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "decoder_vocab_size = len(tokenizer.get_vocab().keys())\n",
    "model = TransformerModel(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    decoder_vocab_size, \n",
    "    pe_target=decoder_vocab_size,\n",
    "    bert=model\n",
    ")\n",
    "model.load_state_dict(torch.load('transformer_epoch_15.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
